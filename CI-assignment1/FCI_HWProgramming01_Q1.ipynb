{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9533189d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights: [-4.53888294e+00 -1.86664859e-01 -8.64233376e-01 -6.16347159e-01\n",
      " -6.86105174e-01 -6.91941208e-01 -2.09427659e-01 -2.65104020e-01\n",
      " -4.72027489e-01  5.50168618e-01 -2.45234164e+00 -1.05335096e+00\n",
      " -4.87382655e-01  7.61241603e-01  1.17601179e+00  8.22200931e-01\n",
      "  1.40717887e-02  1.94237883e+00  4.18307476e-01 -7.89320140e-01\n",
      " -7.33531924e-01 -7.02046706e-01  4.43774595e-01 -2.42361456e-01\n",
      "  3.99754073e-01 -2.92435931e-01 -4.91685135e-01 -1.46036430e+00\n",
      "  4.77527450e-01 -5.70829312e-01  8.84351179e-01 -1.37965323e+00\n",
      " -8.16030665e-01 -1.10872091e+00 -6.91653028e-01 -4.65076299e-02\n",
      " -3.34077778e-01 -5.74434978e-01 -3.05232013e-01 -3.01345698e-01\n",
      " -1.19457833e+00  6.61819987e-01  1.96864836e-01  9.82939585e-02\n",
      "  1.27022077e+00 -5.36664893e-01 -2.30696234e-01 -7.93664057e-01\n",
      "  8.18475080e-02  3.83230262e-01  1.43499051e+00  5.12762718e-01\n",
      "  2.77561358e-01  4.67289356e-01  6.00048446e-01  7.99780628e-01\n",
      "  8.25216863e-01  1.40861604e+00 -8.88024908e-01  9.67703941e-01\n",
      "  3.45869333e-01 -1.94070406e-01 -1.00524231e-01  8.31943767e-01\n",
      " -4.37324171e-01  1.49334028e+00 -1.08303922e+00 -2.90523519e-01\n",
      "  9.29173952e-01  1.90794568e-01 -1.11487949e+00 -5.02158792e-02\n",
      "  2.60860193e-03 -4.31808779e-01  1.22893272e-01  8.19111728e-01\n",
      " -4.32699493e-01 -7.55729322e-01  7.83453304e-01  2.92787432e-01\n",
      " -1.02651117e-01  5.53777750e-01 -1.06436661e+00  1.27877081e+00\n",
      "  1.34929174e-01 -6.47076244e-01 -1.50568568e+00  3.74259250e-01\n",
      " -4.19049027e-01  2.14901998e-01  6.05806959e-01 -3.04355307e-01\n",
      "  5.08159058e-01 -2.80818392e-01  1.68818569e-01 -2.03272226e-01\n",
      " -1.38670003e+00 -7.69841931e-01  1.11986371e-01  1.12352367e+00\n",
      "  1.37332136e+00  6.08902035e-01 -1.71848754e-01  5.39075999e-01\n",
      " -1.49767691e+00 -2.42516496e+00  3.51263053e-01 -8.99170733e-02\n",
      " -2.05328910e-01 -4.97504021e-01 -1.52082626e-01 -3.81624107e-02\n",
      "  2.69049382e-01 -1.36792292e+00  1.25896364e+00  7.00504282e-01\n",
      " -1.15757565e-02  3.47260308e-01  9.06177401e-01 -1.91535691e-01\n",
      "  3.17435981e-01  7.79866582e-01 -1.43195746e+00 -3.09877078e-02\n",
      "  1.86897688e-01 -1.10699872e-01 -1.08945998e+00  8.22264421e-01\n",
      " -1.18626353e+00  2.99454493e-01  1.42075012e-01 -6.98276765e-01\n",
      "  1.16093314e-01 -3.26735302e-01  4.46796974e-01 -9.01674009e-01\n",
      " -1.87319273e-01 -5.27861027e-01 -5.61792144e-01  8.29731796e-01\n",
      "  1.09049754e-01 -1.09401403e+00 -1.82608885e-01 -8.77715729e-01\n",
      "  3.84241504e-01 -3.02405893e-01 -4.63983966e-02 -8.16543855e-01\n",
      " -6.08659739e-02  1.99253322e-02  1.51211759e+00  5.09674303e-01\n",
      " -4.75480921e-01 -5.68347939e-02  7.72665891e-01  3.07046015e-01\n",
      " -1.14697794e-01 -6.16416729e-01  7.65235282e-01  6.68642470e-01\n",
      " -3.73721116e-02  2.98858675e-01 -1.31327041e+00 -8.30303132e-02\n",
      " -7.64071489e-01  8.38061250e-02 -2.21362997e-01]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, learning_rate=0.1, num_iterations=1000, add_intercept=True):\n",
    "    \"\"\"logistic regression with gradient descent\"\"\"\n",
    "    if add_intercept:\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))  # add intercept column\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.random.randn(n_features)  # initialize weights randomly\n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, weights)\n",
    "        y_pred = sigmoid(z)\n",
    "        error = y_pred - y\n",
    "        gradient = np.dot(X.T, error) / n_samples\n",
    "        weights -= learning_rate * gradient\n",
    "    return weights\n",
    "\n",
    "# read in data from .data file\n",
    "data = pd.read_csv('clean2.data', header=None, delimiter=',', index_col=False)\n",
    "data = data.drop(columns=data.columns[:2])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit scaler on data\n",
    "scaler.fit(data.iloc[:, :-1])  # assumes last column is the target variable\n",
    "\n",
    "# transform data\n",
    "data.iloc[:, :-1] = scaler.transform(data.iloc[:, :-1])\n",
    "# preprocess data to extract input features and binary labels\n",
    "X = data.iloc[:, :-1].values  # input features\n",
    "y = (data.iloc[:, -1].values == 1).astype(int)  # binary labels\n",
    "\n",
    "# run logistic regression with gradient descent\n",
    "learned_weights = logistic_regression(X, y)\n",
    "\n",
    "print(\"Learned weights:\", learned_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4dbd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('clean1.data', header=None, delimiter=',', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb5b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=test.columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# fit scaler on data\n",
    "scaler.fit(test.iloc[:, :-1])  # assumes last column is the target variable\n",
    "\n",
    "# transform data\n",
    "test.iloc[:, :-1] = scaler.transform(test.iloc[:, :-1])\n",
    "# preprocess data to extract input features and binary labels\n",
    "X_test = test.iloc[:, :-1].values  # input features\n",
    "y_test = (test.iloc[:, -1].values == 1).astype(int)  # binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0fe278",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e482db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.707983193277311\n",
      "Precision: 0.84\n",
      "Recall: 0.4057971014492754\n",
      "F1 score: 0.5472312703583062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# assume you have the sigmoid of the learned weights multiplied by the test data\n",
    "sigmoid_values = sigmoid(np.dot(X_test,learned_weights))\n",
    "# assume you have the true labels of the test data\n",
    "true_labels = y_test\n",
    "\n",
    "# convert sigmoid values to binary predictions using a threshold of 0.5\n",
    "binary_predictions = (sigmoid_values >= 0.5).astype(int)\n",
    "\n",
    "# calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "precision = precision_score(true_labels, binary_predictions)\n",
    "recall = recall_score(true_labels, binary_predictions)\n",
    "f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "# print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667565b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9201273113064565\n",
      "Precision: 0.7930622009569378\n",
      "Recall: 0.6519174041297935\n",
      "F1 score: 0.7155963302752293\n"
     ]
    }
   ],
   "source": [
    "# assume you have the sigmoid of the learned weights multiplied by the test data\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "sigmoid_values = sigmoid(np.dot(X,learned_weights))\n",
    "# assume you have the true labels of the test data\n",
    "true_labels = y\n",
    "\n",
    "# convert sigmoid values to binary predictions using a threshold of 0.5\n",
    "binary_predictions = (sigmoid_values >= 0.5).astype(int)\n",
    "\n",
    "# calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "precision = precision_score(true_labels, binary_predictions)\n",
    "recall = recall_score(true_labels, binary_predictions)\n",
    "f1 = f1_score(true_labels, binary_predictions)\n",
    "\n",
    "# print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01edc31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
